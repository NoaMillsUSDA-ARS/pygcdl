{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b309cf5c",
   "metadata": {},
   "source": [
    "This document includes code to test and document various pygcdl functions. Before running the code in this document, run the code in \"sample_data/create_spatial_data.ipynb\" to create the data files that this code uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5eaa296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pygcdl' from 'C:\\\\Users\\\\Noa.Mills\\\\Documents\\\\pygcdl\\\\pygcdl.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we import the necessary libraries\n",
    "import sys\n",
    "import pygcdl\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import importlib\n",
    "importlib.reload(pygcdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b666a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pygcdl object\n",
    "# url_base set for local development\n",
    "# Remove url_base for Ceres development and testing\n",
    "pygcdl_obj = pygcdl.PyGeoCDL(url_base=\"http://127.0.0.1:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e67ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DaymetV4': 'Daymet Version 4', 'GTOPO30': 'Global 30 Arc-Second Elevation', 'MODIS_NDVI': 'MODIS NDVI Data, Smoothed and Gap-filled, for the Conterminous US: 2000-2015', 'NASS_CDL': 'NASS Cropland Data Layer', 'NLCD': 'National Land Cover Database', 'PRISM': 'PRISM', 'RAPV3': 'Rangeland Analysis Platform Version 3', 'SMAP-HB1km': 'SMAP HydroBlocks - 1 km', 'Soilgrids250mV2': 'SoilGrids â€” global gridded soil information', 'VIP': 'Vegetation Index and Phenology (VIP) Vegetation Indices Daily Global 0.05Deg CMG V004'}\n"
     ]
    }
   ],
   "source": [
    "print(pygcdl_obj.list_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3489ec24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ppt': 'total precipitation (rain+melted snow)', 'tmean': 'mean temperature (mean of tmin and tmax)', 'tmin': 'minimum temperature', 'tmax': 'maximum temperature', 'tdmean': 'mean dew point temperature', 'vpdmin': 'minimum vapor pressure deficit', 'vpdmax': 'maximum vapor pressure deficit'}\n"
     ]
    }
   ],
   "source": [
    "print(pygcdl_obj.get_dataset_info(\"PRISM\")[\"vars\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1de97e",
   "metadata": {},
   "source": [
    "We can upload a geometry as:\n",
    "- A geojson file\n",
    "- A shapefile\n",
    "- A zipfile containing shapefile files\n",
    "- A csv file (point data only?)\n",
    "- A geopandas dataframe\n",
    "\n",
    "The GCDL can only handle generate polygon subsets of single polygons, or multipolygon objects that contain only one polygon. If the user attempts to upload a geopandas dataframe that contains multiple polygons, then pygcdl calculates the ratio between the area of the union of polygons, and the area of the convex hull. If the union of polygons covers at least 80% of the area of the convex hull, then the pygcdl uploads the convex hull. Otherwise, pygcdl uploads each polygon individually, and returns a list of GUIDs.\n",
    "\n",
    "If the user uploads a file, the file contents are not checked. So, it is possible for a user to upload a multipolygon file without any errors or warnings, and then run into errors when trying to use the GUID for that upload to download a polygon subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014757df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location of sample data files\n",
    "sample_data_dir = Path(\"sample_data/output_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1536ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f321a02a-5a58-49c1-95a8-8ceca98a9a62\n",
      "f92e1fc4-6e6a-4c9c-b897-d5ea9c372ddb\n",
      "9169d66f-beda-4b57-80aa-9dfc7bb9b2f5\n",
      "35dc9b8a-836b-4995-8166-18a8793623b3\n",
      "08c9204b-4b8f-454c-a19f-31fa139ecff8\n",
      "b70ab7a6-5b0a-4d3c-99e7-142e779148e9\n",
      "145ec8fb-3219-4343-86a2-203e9f0e93ca\n",
      "9f2574d8-cd6c-42cb-94e5-0e19fe0d7bca\n"
     ]
    }
   ],
   "source": [
    "# Upload polygon shapefiles\n",
    "subset_counties1_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties1.zip\")\n",
    "subset_counties2_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties2.zip\")\n",
    "subset_counties3_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties3.zip\")\n",
    "subset_counties4_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties4.zip\")\n",
    "subset_counties5_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties5.zip\")\n",
    "subset_counties6_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties6.zip\")\n",
    "subset_counties7_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties7.zip\")\n",
    "subset_counties8_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties8.zip\")\n",
    "print(subset_counties1_guid)\n",
    "print(subset_counties2_guid)\n",
    "print(subset_counties3_guid)\n",
    "print(subset_counties4_guid)\n",
    "print(subset_counties5_guid)\n",
    "print(subset_counties6_guid)\n",
    "print(subset_counties7_guid)\n",
    "print(subset_counties8_guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e29692",
   "metadata": {},
   "source": [
    "Upload a polygon .shp file. This finds associated files (ie .shp, .cpg, etc) and creates a zip file of them. Since all of our shapefiles are already in zip files, we will first unzip a .zip file, then call upload_geometry on the .shp file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95fcf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths to zip file, and path to unzip those files into\n",
    "path_to_zip_file = Path(sample_data_dir / \"subset_counties1.zip\")\n",
    "upload_shp_dir = Path(sample_data_dir / \"upload_shp_dir\")\n",
    "upload_shp_dir.mkdir(exist_ok=True)\n",
    "# Unzip subset_counties1.zip files into upload_shp_dir\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path=upload_shp_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "389d4092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1edd0164-8d5d-427e-adec-c88b28686ffd\n"
     ]
    }
   ],
   "source": [
    "subset_counties1_shp_guid = pygcdl_obj.upload_geometry(upload_shp_dir / \"subset_counties1.shp\")\n",
    "print(subset_counties1_shp_guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2424357",
   "metadata": {},
   "source": [
    "If you observe your file system, you will now see that you have the following files in sample_data/output_data/upload_shp_dir:\n",
    "- subset_counties1.cpg\n",
    "- subset_counties1.dbf\n",
    "- subset_counties1.prj\n",
    "- subset_counties1.shp\n",
    "- subset_counties1.shx\n",
    "- subset_counties1.zip\n",
    "We asked pygcdl to upload `subset_counties1.shp`, so it identified the minimum related files (.shp, .shx, .dbf, .prj), created the zipfile `subset_counties1.zip` containing these files, and uploaded that zipfile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63450299",
   "metadata": {},
   "source": [
    "Now, we will show what happens if you attempt to upload a polygon .shp file that doesn't have the associated files in the same directory. First, we will remove all files except for `subset_counties1.shp` from sample_data/output_data/upload_shp_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad5b57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all files except for subset_counties1.shp\n",
    "ext_to_remove = [\".cpg\", \".dbf\", \".prj\", \".shx\", \".zip\"]\n",
    "base_file = upload_shp_dir / \"subset_counties1\"\n",
    "for ext in ext_to_remove:\n",
    "    base_file.with_suffix(ext).unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d420c92",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: WindowsPath('C:/Users/Noa.Mills/Documents/pygcdl/sample_data/output_data/upload_shp_dir/subset_counties1.shx')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# CAUSES AN ERROR\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Attempt to upload the lonely shapefile, and observe the error produced by pygcdl\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m faulty_guid \u001b[38;5;241m=\u001b[39m \u001b[43mpygcdl_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupload_shp_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubset_counties1.shp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\pygcdl\\pygcdl.py:69\u001b[0m, in \u001b[0;36mPyGeoCDL.upload_geometry\u001b[1;34m(self, geom)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeom_guid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.shp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Call utility function to zip all auxillary shapefile files \u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# together\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     zip_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_zip_shapefiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeom\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     71\u001b[0m     files \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeom_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: (zip_file_path, \u001b[38;5;28mopen\u001b[39m(zip_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))}\n\u001b[0;32m     72\u001b[0m     r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl_base \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/upload_geom\u001b[39m\u001b[38;5;124m'\u001b[39m, files\u001b[38;5;241m=\u001b[39mfiles)\n",
      "File \u001b[1;32m~\\Documents\\pygcdl\\pygcdl.py:395\u001b[0m, in \u001b[0;36mPyGeoCDL._zip_shapefiles\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    393\u001b[0m         new_file \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mwith_suffix(suffix)\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_file\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m--> 395\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mENOENT, os\u001b[38;5;241m.\u001b[39mstrerror(errno\u001b[38;5;241m.\u001b[39mENOENT), new_file)\n\u001b[0;32m    396\u001b[0m         z\u001b[38;5;241m.\u001b[39mwrite(new_file, arcname\u001b[38;5;241m=\u001b[39mnew_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(output_zip_dir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: WindowsPath('C:/Users/Noa.Mills/Documents/pygcdl/sample_data/output_data/upload_shp_dir/subset_counties1.shx')"
     ]
    }
   ],
   "source": [
    "# CAUSES AN ERROR\n",
    "# Attempt to upload the lonely shapefile, and observe the error produced by pygcdl\n",
    "faulty_guid = pygcdl_obj.upload_geometry(upload_shp_dir / \"subset_counties1.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77646fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92531d94-e941-46d5-bbdf-b05313f6ff55\n",
      "e80c2e94-7fb6-42c6-a7af-b14070411979\n"
     ]
    }
   ],
   "source": [
    "# Upload polygon geojson file\n",
    "subset_counties1_geojson_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties1.geojson\")\n",
    "subset_counties2_geojson_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"subset_counties2.geojson\")\n",
    "print(subset_counties1_geojson_guid)\n",
    "print(subset_counties2_geojson_guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32067e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ba6cbc39-31cf-40c4-8b67-0cd10856fd0c\n"
     ]
    }
   ],
   "source": [
    "# Upload a points shapefile zipfile\n",
    "county_centroids_shp_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"county_centroids.zip\")\n",
    "print(county_centroids_shp_guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd99e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9d770c49-95c2-407a-9dcc-c3e0f38a10a7\n"
     ]
    }
   ],
   "source": [
    "# Upload a points geojson\n",
    "county_centroids_geojson_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"county_centroids.geojson\")\n",
    "print(county_centroids_geojson_guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4420394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528b2fd6-02bb-4db4-aef7-07f12dcaf909\n"
     ]
    }
   ],
   "source": [
    "# Upload a points csv file\n",
    "county_centroids_csv_guid = pygcdl_obj.upload_geometry(sample_data_dir / \"county_centroids.geojson\")\n",
    "print(county_centroids_csv_guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43eed2d",
   "metadata": {},
   "source": [
    "Now that we have covered the several different ways to upload data, and uploaded various data files for testing, let's take a look at how to download subsets of data. First we specify the datasets and variables we would like to use. We can do this with a pandas dataframe, a dict, or a matrix as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1da7ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      dataset variable\n",
      "0       PRISM      ppt\n",
      "1  MODIS_NDVI     NDVI\n"
     ]
    }
   ],
   "source": [
    "# Specify datasets and variables as a pandas dataframe\n",
    "dsvars1 = pd.DataFrame(\n",
    "    [[\"PRISM\", \"ppt\"], [\"MODIS_NDVI\", \"NDVI\"]], \n",
    "    columns=[\"dataset\", \"variable\"])\n",
    "print(dsvars1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da5e3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify datasets and variables as a dict\n",
    "dsvars2 = {\"PRISM\":[\"ppt\", \"tmean\"], \"MODIS_NDVI\":[\"NDVI\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd770c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify datasets and variables as a list\n",
    "dsvars3 = [[\"PRISM\", \"ppt\"],[\"PRISM\", \"tmean\"], [\"MODIS_NDVI\", \"NDVI\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a68514f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['PRISM', 'ppt'],\n",
       "       ['PRISM', 'tmean'],\n",
       "       ['MODIS_NDVI', 'NDVI']], dtype='<U10')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify datasets and variables as a numpy array\n",
    "dsvars4 = np.array([[\"PRISM\", \"ppt\"],[\"PRISM\", \"tmean\"], [\"MODIS_NDVI\", \"NDVI\"]])\n",
    "dsvars4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97256af3",
   "metadata": {},
   "source": [
    "Next, we specify our date data and grain method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b970330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = \"2008\"\n",
    "months = \"7:8\"\n",
    "grain_method = \"any\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc5611",
   "metadata": {},
   "source": [
    "Then, we specify our spatial resolution and resampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ade443ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spat_res = 1000 # in units of meters\n",
    "resample_method = \"bilinear\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed0631",
   "metadata": {},
   "source": [
    "Lastly, we construct a directory for our downloaded files to go to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a13ee07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"output_test\")\n",
    "if not output_path.is_dir():\n",
    "    output_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b601d4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8000/subset_polygon?datasets=PRISM%3Appt%2Ctmean%3BMODIS_NDVI%3ANDVI&geom_guid=b70ab7a6-5b0a-4d3c-99e7-142e779148e9&resample_method=nearest&years=2008&months=7&grain_method=any&validate_method=strict&output_format=geotiff\n",
      "Files downloaded and unzipped:  ['metadata.json', 'PRISM_ppt_2008-07.tif', 'PRISM_tmean_2008-07.tif', 'MODIS_NDVI_NDVI_2008-07-07.tif', 'MODIS_NDVI_NDVI_2008-07-15.tif', 'MODIS_NDVI_NDVI_2008-07-23.tif', 'MODIS_NDVI_NDVI_2008-07-31.tif']\n"
     ]
    }
   ],
   "source": [
    "output_files = pygcdl_obj.download_polygon_subset(\n",
    "    dsvars=dsvars4, \n",
    "    years=years,\n",
    "    months=months,\n",
    "    grain_method=grain_method,\n",
    "    t_geom=subset_counties6_guid,\n",
    "    dsn=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83b8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
